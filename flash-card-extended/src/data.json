[
  {
    "question": "What does LLM stand for?",
    "answer": "Large Language Model",
    "image": "https://media.expert.ai/expertai/uploads/2023/05/AdobeStock_595600270-1024x768-1.jpg",
    "difficulty": "easy"
  },
  {
    "question": "What is the difference between supervised and unsupervised learning in LLMs?",
    "answer": "Labeled vs. Unlabeled",
    "image": "https://images.prismic.io/encord/f1fa13a6-88a3-4c20-b620-46489fe00f45_What+is+Supervised+Learning+%7C+Encord.png?auto=compress,format",
    "difficulty": "medium"
  },
  {
    "question": "What is the 'attention mechanism' in transformer-based LLMs?",
    "answer": "Weighs word importance",
    "image": "https://media.geeksforgeeks.org/wp-content/uploads/20200603211336/attn.png",
    "difficulty": "hard"
  },
  {
    "question": "What is 'prompt engineering'?",
    "answer": "Optimizing input prompts",
    "image": "https://promptengineering.org/content/images/2023/09/What-is-Prompt-Engineering.png",
    "difficulty": "medium"
  },
  {
    "question": "What is 'tokenization' in the context of LLMs?",
    "answer": "Breaking text into tokens",
    "image": "https://images.ctfassets.net/lzny33ho1g45/xxBv48FmBSnpZTdMPtO1o/ae93a2be970fe66292889e9a7cfb041f/perplexity-app-tips-hero.jpg?w=1520&fm=jpg&q=31&fit=thumb&h=760",
    "difficulty": "easy"
  },
  {
    "question": "What is 'fine-tuning' in the context of LLMs?",
    "answer": "Further training model",
    "image": "https://cdn.prod.website-files.com/614c82ed388d53640613982e/66bb667edb61a05af03b265b_65b7a73f7ee95b6c9e261863_using-prompts-to-fine-tune-llms-with-instruction.webp",
    "difficulty": "medium"
  },
  {
    "question": "What is 'perplexity' in language models?",
    "answer": "Prediction measurement",
    "image": "https://miro.medium.com/v2/resize:fit:1200/1*h4qCgiWk3CaWTQdWrjq0Bw.png",
    "difficulty": "medium"
  },
  {
    "question": "What is 'hallucination' in the context of LLMs?",
    "answer": "Incorrect generated info",
    "image": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLnR20EjCr3rxhLqQxdDGsVsP4bcOpZtxJaA&s",
    "difficulty": "hard"
  },
  {
    "question": "What is the difference between GPT-3 and GPT-4?",
    "answer": "More parameters, better reasoning",
    "image": "https://i0.wp.com/easypromptguide.com/wp-content/uploads/2023/08/openais-gpt-4.jpg",
    "difficulty": "hard"
  },
  {
    "question": "What is 'zero-shot learning' in LLMs?",
    "answer": "No training examples",
    "image": "https://images.prismic.io/encord/9916d1b4-0301-445a-9ddf-0ce7d49b7e58_Zero+Shot+learning.png?auto=compress%2Cformat&fit=max",
    "difficulty": "easy"
  },
  {
    "question": "What is 'RLHF' in the context of LLMs?",
    "answer": "Human feedback alignment",
    "image": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/reward-model.png",
    "difficulty": "easy"
  },
  {
    "question": "What is 'embeddings' in the context of LLMs?",
    "answer": "Semantic vector representations",
    "image": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR8y1kOqF-Cie6bHV7OwG7EqIAq1r0Kof-uiA&s",
    "difficulty": "medium"
  },
  {
    "question": "What is 'context window' in LLMs?",
    "answer": "Maximum text processed",
    "image": "https://cloudkitect.com/wp-content/uploads/2024/09/ContextWindowLimitation-1024x629.webp",
    "difficulty": "medium"
  },
  {
    "question": "What is 'parameter efficient fine-tuning' (PEFT)?",
    "answer": "Efficient fine-tuning techniques",
    "image": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRBtTpC4nJbWibRIHjadDsjM0SALoicnPXEsA&s",
    "difficulty": "hard"
  },
  {
    "question": "What is 'few-shot learning' in LLMs?",
    "answer": "Learning from few examples",
    "image": "https://miro.medium.com/v2/resize:fit:1400/1*4DTL5q--UxuoMFc6P0b3dw.png",
    "difficulty": "easy"
  }
]